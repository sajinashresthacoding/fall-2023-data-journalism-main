{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a089baa",
   "metadata": {},
   "source": [
    "# 1. Demo downloading files from websites \n",
    "\n",
    "There are ```txt``` and ```pdf``` files on:\n",
    "\n",
    "```https://sandeepmj.github.io/scrape-example-page/pages.html```\n",
    "\n",
    "Do the following:\n",
    "\n",
    "1. Download all ```pdf``` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2151394",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create cells here AS needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3ab62ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import all the things we need\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "from random import randrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a28c76dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in /Users/sajinashrestha/opt/anaconda3/lib/python3.9/site-packages (3.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7b0bb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e4b64dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's create a function to get the url we are working with\n",
    "\n",
    "def mk_request(url):\n",
    "    '''\n",
    "    Takes the url and gives you status and returns responses\n",
    "    '''\n",
    "    response = requests.get(url)\n",
    "    if 200 <= response.status_code < 400:\n",
    "        return response\n",
    "    else:\n",
    "        print(f\"request returned {response.status_code}error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b8f003a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's store the url\n",
    "\n",
    "myurl = \"https://sandeepmj.github.io/scrape-example-page/pages.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f61182e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## let's try out the function, see if it works\n",
    "\n",
    "mk_request(myurl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e170eb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's store the response so that we can try a different function\n",
    "response = mk_request(myurl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ba4fecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's create a function for making soup\n",
    "\n",
    "def mk_soup(response):\n",
    "    '''\n",
    "    makes soup out of the response you give\n",
    "    '''\n",
    "    return BeautifulSoup(response.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd81371c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html lang=\"en\">\n",
       "<head>\n",
       "<!-- Makes the page responsive and scaled to be read easily -->\n",
       "<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n",
       "<!-- Links to stylesheet -->\n",
       "<link href=\"style.css\" rel=\"stylesheet\" type=\"text/css\"/>\n",
       "<!-- Remember to update page title -->\n",
       "<title>List of Documents</title>\n",
       "</head>\n",
       "<body>\n",
       "<!-- All content goes here -->\n",
       "<div class=\"container\">\n",
       "<h1>Documents to Download</h1>\n",
       "<li>Junk Li <a href=\"\">tag 1</a></li>\n",
       "<li>Junk Li <a href=\"\">tag 2</a></li>\n",
       "<ul class=\"txts downloadable\">\n",
       "<p class=\"pages\">Download this first set of text documents</p>\n",
       "<li>Text Document <a href=\"files/text_doc_01.txt\">1</a> </li>\n",
       "<li>Text Document <a href=\"files/text_doc_02.txt\">2</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_03.txt\">3</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_04.txt\">4</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_05.txt\">5</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_06.txt\">6</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_07.txt\">7</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_08.txt\">8</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_09.txt\">9</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_10.txt\">10</a></li>\n",
       "</ul>\n",
       "<li>Junk Li <a href=\"\">tag 3</a></li>\n",
       "<li>Junk Li <a href=\"\">tag 4</a></li>\n",
       "<ul class=\"pdfs downloadable\">\n",
       "<p class=\"pages\">Download this list of PDFs</p>\n",
       "<li>PDF Document <a href=\"files/pdf_1.pdf\">1</a> </li>\n",
       "<li>PDF Document <a href=\"files/pdf_2.pdf\">2</a></li>\n",
       "<li>PDF Document <a href=\"files/pdf_3.pdf\">3</a></li>\n",
       "<li>PDF Document <a href=\"files/pdf_4.pdf\">4</a></li>\n",
       "<li>PDF Document <a href=\"files/pdf_5.pdf\">5</a></li>\n",
       "<li>PDF Document <a href=\"files/pdf_6.pdf\">6</a></li>\n",
       "<li>PDF Document <a href=\"files/pdf_7.pdf\">7</a></li>\n",
       "<li>PDF Document <a href=\"files/pdf_8.pdf\">8</a></li>\n",
       "<li>PDF Document <a href=\"files/pdf_9.pdf\">9</a></li>\n",
       "<li>PDF Document <a href=\"files/pdf_10.pdf\">10</a></li>\n",
       "</ul>\n",
       "<li>Junk Li <a href=\"\">tag 5</a></li>\n",
       "<li>Junk Li <a href=\"\">tag 6</a></li>\n",
       "<ul class=\"txts downloadable\">\n",
       "<p class=\"pages\">Download this second set of text documents</p>\n",
       "<li>Text Document <a href=\"files/text_doc_A.txt\">1</a> </li>\n",
       "<li>Text Document <a href=\"files/text_doc_B.txt\">2</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_C.txt\">3</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_D.txt\">4</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_E.txt\">5</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_F.txt\">6</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_G.txt\">7</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_H.txt\">8</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_I.txt\">9</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_J.txt\">10</a></li>\n",
       "</ul>\n",
       "</div>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## let's test the function\n",
    "\n",
    "soup = mk_soup(response)\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4b8a396",
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's make a mother function that can do both these functions at once for future use, unclogs code\n",
    "\n",
    "def scrapes(url):\n",
    "    '''\n",
    "    give it a url to return docsoup of a page\n",
    "    '''\n",
    "    return mk_soup(mk_request(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eeb40c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html lang=\"en\">\n",
       "<head>\n",
       "<!-- Makes the page responsive and scaled to be read easily -->\n",
       "<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n",
       "<!-- Links to stylesheet -->\n",
       "<link href=\"style.css\" rel=\"stylesheet\" type=\"text/css\"/>\n",
       "<!-- Remember to update page title -->\n",
       "<title>List of Documents</title>\n",
       "</head>\n",
       "<body>\n",
       "<!-- All content goes here -->\n",
       "<div class=\"container\">\n",
       "<h1>Documents to Download</h1>\n",
       "<li>Junk Li <a href=\"\">tag 1</a></li>\n",
       "<li>Junk Li <a href=\"\">tag 2</a></li>\n",
       "<ul class=\"txts downloadable\">\n",
       "<p class=\"pages\">Download this first set of text documents</p>\n",
       "<li>Text Document <a href=\"files/text_doc_01.txt\">1</a> </li>\n",
       "<li>Text Document <a href=\"files/text_doc_02.txt\">2</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_03.txt\">3</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_04.txt\">4</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_05.txt\">5</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_06.txt\">6</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_07.txt\">7</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_08.txt\">8</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_09.txt\">9</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_10.txt\">10</a></li>\n",
       "</ul>\n",
       "<li>Junk Li <a href=\"\">tag 3</a></li>\n",
       "<li>Junk Li <a href=\"\">tag 4</a></li>\n",
       "<ul class=\"pdfs downloadable\">\n",
       "<p class=\"pages\">Download this list of PDFs</p>\n",
       "<li>PDF Document <a href=\"files/pdf_1.pdf\">1</a> </li>\n",
       "<li>PDF Document <a href=\"files/pdf_2.pdf\">2</a></li>\n",
       "<li>PDF Document <a href=\"files/pdf_3.pdf\">3</a></li>\n",
       "<li>PDF Document <a href=\"files/pdf_4.pdf\">4</a></li>\n",
       "<li>PDF Document <a href=\"files/pdf_5.pdf\">5</a></li>\n",
       "<li>PDF Document <a href=\"files/pdf_6.pdf\">6</a></li>\n",
       "<li>PDF Document <a href=\"files/pdf_7.pdf\">7</a></li>\n",
       "<li>PDF Document <a href=\"files/pdf_8.pdf\">8</a></li>\n",
       "<li>PDF Document <a href=\"files/pdf_9.pdf\">9</a></li>\n",
       "<li>PDF Document <a href=\"files/pdf_10.pdf\">10</a></li>\n",
       "</ul>\n",
       "<li>Junk Li <a href=\"\">tag 5</a></li>\n",
       "<li>Junk Li <a href=\"\">tag 6</a></li>\n",
       "<ul class=\"txts downloadable\">\n",
       "<p class=\"pages\">Download this second set of text documents</p>\n",
       "<li>Text Document <a href=\"files/text_doc_A.txt\">1</a> </li>\n",
       "<li>Text Document <a href=\"files/text_doc_B.txt\">2</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_C.txt\">3</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_D.txt\">4</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_E.txt\">5</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_F.txt\">6</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_G.txt\">7</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_H.txt\">8</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_I.txt\">9</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_J.txt\">10</a></li>\n",
       "</ul>\n",
       "</div>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = scrapes(myurl)\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c44d7b7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"files/pdf_1.pdf\">1</a>,\n",
       " <a href=\"files/pdf_2.pdf\">2</a>,\n",
       " <a href=\"files/pdf_3.pdf\">3</a>,\n",
       " <a href=\"files/pdf_4.pdf\">4</a>,\n",
       " <a href=\"files/pdf_5.pdf\">5</a>,\n",
       " <a href=\"files/pdf_6.pdf\">6</a>,\n",
       " <a href=\"files/pdf_7.pdf\">7</a>,\n",
       " <a href=\"files/pdf_8.pdf\">8</a>,\n",
       " <a href=\"files/pdf_9.pdf\">9</a>,\n",
       " <a href=\"files/pdf_10.pdf\">10</a>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## find where the pdf docs are being held\n",
    "aTags = soup.find(\"ul\", class_=\"pdfs\").find_all(\"a\")\n",
    "aTags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45a08825",
   "metadata": {},
   "outputs": [],
   "source": [
    "## establish a base url\n",
    "\n",
    "base_url = \"https://sandeepmj.github.io/scrape-example-page/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4490857e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://sandeepmj.github.io/scrape-example-page/files/pdf_1.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_2.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_3.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_4.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_5.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_6.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_7.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_8.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_9.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_10.pdf']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get all the links\n",
    "\n",
    "pdf_links = [base_url + aTag.get(\"href\")for aTag in aTags]\n",
    "pdf_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd9fe1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## now that we have pdf links, let's download all the pdf files\n",
    "## first make snoozer for it\n",
    "\n",
    "def snoozer(start_range, end_range):\n",
    "    snooze_time = randrange(start_range, end_range)\n",
    "    print(f\"\\n snoozing for {snooze_time}seconds\")\n",
    "    return time.sleep(snooze_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "31359515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading link 1 of 10\n",
      "100% [..........................................................] 12812 / 12812\n",
      " snoozing for 22seconds\n",
      "Downloading link 2 of 10\n",
      "100% [..........................................................] 12897 / 12897\n",
      " snoozing for 15seconds\n",
      "Downloading link 3 of 10\n",
      "100% [..........................................................] 12908 / 12908\n",
      " snoozing for 22seconds\n",
      "Downloading link 4 of 10\n",
      "100% [..........................................................] 12843 / 12843\n",
      " snoozing for 17seconds\n",
      "Downloading link 5 of 10\n",
      "100% [..........................................................] 12881 / 12881\n",
      " snoozing for 25seconds\n",
      "Downloading link 6 of 10\n",
      "100% [..........................................................] 12906 / 12906\n",
      " snoozing for 25seconds\n",
      "Downloading link 7 of 10\n",
      "100% [..........................................................] 12816 / 12816\n",
      " snoozing for 19seconds\n",
      "Downloading link 8 of 10\n",
      "100% [..........................................................] 12921 / 12921\n",
      " snoozing for 17seconds\n",
      "Downloading link 9 of 10\n",
      "100% [..........................................................] 12901 / 12901\n",
      " snoozing for 24seconds\n",
      "Downloading link 10 of 10\n",
      "100% [..........................................................] 13049 / 13049\n",
      " snoozing for 18seconds\n"
     ]
    }
   ],
   "source": [
    "## now scrape!\n",
    "\n",
    "link_count = 1\n",
    "start_range, end_range = 15, 30\n",
    "for link in pdf_links: \n",
    "    print(f\"Downloading link {link_count} of {len(pdf_links)}\")\n",
    "    link_count +=1\n",
    "    wget.download(link)\n",
    "    snoozer(start_range, end_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc37c3dc",
   "metadata": {},
   "source": [
    "# 2. Demo downloading ALL files from websites \n",
    "\n",
    "There are ```txt``` and ```pdf``` files on:\n",
    "\n",
    "```https://sandeepmj.github.io/scrape-example-page/pages.html```\n",
    "\n",
    "Do the following:\n",
    "\n",
    "1. Download all  files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d786775f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"files/text_doc_01.txt\">1</a>,\n",
       " <a href=\"files/text_doc_02.txt\">2</a>,\n",
       " <a href=\"files/text_doc_03.txt\">3</a>,\n",
       " <a href=\"files/text_doc_04.txt\">4</a>,\n",
       " <a href=\"files/text_doc_05.txt\">5</a>,\n",
       " <a href=\"files/text_doc_06.txt\">6</a>,\n",
       " <a href=\"files/text_doc_07.txt\">7</a>,\n",
       " <a href=\"files/text_doc_08.txt\">8</a>,\n",
       " <a href=\"files/text_doc_09.txt\">9</a>,\n",
       " <a href=\"files/text_doc_10.txt\">10</a>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create cells here AS needed\n",
    "\n",
    "## tried to download all the downloadables at once so that it captures both txt and pds\n",
    "### but it didnt return all, just txts, idk why :(\n",
    "\n",
    "all_tags = soup.find(\"ul\", class_=\"downloadable\").find_all(\"a\")\n",
    "all_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b96e055e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"files/text_doc_01.txt\">1</a>,\n",
       " <a href=\"files/text_doc_02.txt\">2</a>,\n",
       " <a href=\"files/text_doc_03.txt\">3</a>,\n",
       " <a href=\"files/text_doc_04.txt\">4</a>,\n",
       " <a href=\"files/text_doc_05.txt\">5</a>,\n",
       " <a href=\"files/text_doc_06.txt\">6</a>,\n",
       " <a href=\"files/text_doc_07.txt\">7</a>,\n",
       " <a href=\"files/text_doc_08.txt\">8</a>,\n",
       " <a href=\"files/text_doc_09.txt\">9</a>,\n",
       " <a href=\"files/text_doc_10.txt\">10</a>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##use functions to get txt files\n",
    "\n",
    "txt_tags = soup.find(\"ul\", class_=\"txts\").find_all(\"a\")\n",
    "txt_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "651bdaa7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://sandeepmj.github.io/scrape-example-page/files/text_doc_01.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_02.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_03.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_04.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_05.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_06.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_07.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_08.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_09.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_10.txt']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get txt links\n",
    "\n",
    "txt_links = [base_url + txt_tag.get(\"href\")for txt_tag in txt_tags]\n",
    "txt_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aaad07c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading link 1 of 10\n",
      "100% [................................................................] 76 / 76\n",
      " snoozing for 25seconds\n",
      "Downloading link 2 of 10\n",
      "100% [................................................................] 66 / 66\n",
      " snoozing for 24seconds\n",
      "Downloading link 3 of 10\n",
      "100% [................................................................] 70 / 70\n",
      " snoozing for 28seconds\n",
      "Downloading link 4 of 10\n",
      "100% [................................................................] 63 / 63\n",
      " snoozing for 16seconds\n",
      "Downloading link 5 of 10\n",
      "100% [................................................................] 66 / 66\n",
      " snoozing for 20seconds\n",
      "Downloading link 6 of 10\n",
      "100% [................................................................] 66 / 66\n",
      " snoozing for 23seconds\n",
      "Downloading link 7 of 10\n",
      "100% [................................................................] 69 / 69\n",
      " snoozing for 29seconds\n",
      "Downloading link 8 of 10\n",
      "100% [................................................................] 65 / 65\n",
      " snoozing for 23seconds\n",
      "Downloading link 9 of 10\n",
      "100% [................................................................] 66 / 66\n",
      " snoozing for 27seconds\n",
      "Downloading link 10 of 10\n",
      "100% [................................................................] 60 / 60\n",
      " snoozing for 25seconds\n"
     ]
    }
   ],
   "source": [
    "## scrape txt files\n",
    "\n",
    "link_count = 1\n",
    "start_range, end_range = 15, 30\n",
    "for link in txt_links: \n",
    "    print(f\"Downloading link {link_count} of {len(txt_links)}\")\n",
    "    link_count +=1\n",
    "    wget.download(link)\n",
    "    snoozer(start_range, end_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "94a2b6ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"files/pdf_1.pdf\">1</a>,\n",
       " <a href=\"files/pdf_2.pdf\">2</a>,\n",
       " <a href=\"files/pdf_3.pdf\">3</a>,\n",
       " <a href=\"files/pdf_4.pdf\">4</a>,\n",
       " <a href=\"files/pdf_5.pdf\">5</a>,\n",
       " <a href=\"files/pdf_6.pdf\">6</a>,\n",
       " <a href=\"files/pdf_7.pdf\">7</a>,\n",
       " <a href=\"files/pdf_8.pdf\">8</a>,\n",
       " <a href=\"files/pdf_9.pdf\">9</a>,\n",
       " <a href=\"files/pdf_10.pdf\">10</a>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_tags = soup.find(\"ul\", class_=\"pdfs\").find_all(\"a\")\n",
    "pdf_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e11817b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://sandeepmj.github.io/scrape-example-page/files/pdf_1.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_2.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_3.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_4.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_5.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_6.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_7.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_8.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_9.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_10.pdf']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_links = [base_url + pdf_tag.get(\"href\")for pdf_tag in pdf_tags]\n",
    "pdf_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "730826bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading link 1 of 10\n",
      "100% [..........................................................] 12812 / 12812\n",
      " snoozing for 16seconds\n",
      "Downloading link 2 of 10\n",
      "100% [..........................................................] 12897 / 12897\n",
      " snoozing for 26seconds\n",
      "Downloading link 3 of 10\n",
      "100% [..........................................................] 12908 / 12908\n",
      " snoozing for 21seconds\n",
      "Downloading link 4 of 10\n",
      "100% [..........................................................] 12843 / 12843\n",
      " snoozing for 22seconds\n",
      "Downloading link 5 of 10\n",
      "100% [..........................................................] 12881 / 12881\n",
      " snoozing for 17seconds\n",
      "Downloading link 6 of 10\n",
      "100% [..........................................................] 12906 / 12906\n",
      " snoozing for 17seconds\n",
      "Downloading link 7 of 10\n",
      "100% [..........................................................] 12816 / 12816\n",
      " snoozing for 15seconds\n",
      "Downloading link 8 of 10\n",
      "100% [..........................................................] 12921 / 12921\n",
      " snoozing for 28seconds\n",
      "Downloading link 9 of 10\n",
      "100% [..........................................................] 12901 / 12901\n",
      " snoozing for 26seconds\n",
      "Downloading link 10 of 10\n",
      "100% [..........................................................] 13049 / 13049\n",
      " snoozing for 19seconds\n"
     ]
    }
   ],
   "source": [
    "## scrape pdf files\n",
    "\n",
    "link_count = 1\n",
    "start_range, end_range = 15, 30\n",
    "for link in pdf_links: \n",
    "    print(f\"Downloading link {link_count} of {len(pdf_links)}\")\n",
    "    link_count +=1\n",
    "    wget.download(link)\n",
    "    snoozer(start_range, end_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9faeae",
   "metadata": {},
   "source": [
    "# 3. Conversion function\n",
    "\n",
    "\n",
    "Write a function that takes string values like ```$12.24```, ```10,201.7654``` and ```$12,501``` and converts them into floating point numbers like ```12.24```, ```10201.77``` and ```12501.0```\n",
    "\n",
    "Test it out on those 3 string values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38e6e42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "ca97e315",
   "metadata": {},
   "outputs": [],
   "source": [
    "## write function here\n",
    "\n",
    "def test(number):\n",
    "    '''\n",
    "    Takes string values and turns them into floats\n",
    "    '''\n",
    "    if '$' and ',' in number:\n",
    "        hold = number.replace(\"$\", \"\")\n",
    "        hold2 = hold.replace(\",\", \"\")\n",
    "        answer = float(str(hold2))\n",
    "        print(f\"The float number is {answer}\")\n",
    "    elif'$' in number:\n",
    "        holder = float(str(number).replace(\"$\", \"\"))\n",
    "        answer = round(holder, 2)\n",
    "        print(f\"The float number is {answer}\")\n",
    "    elif ',' in number:\n",
    "        holder = float(str(number).replace(\",\", \"\"))\n",
    "        answer = round(holder, 2)\n",
    "        print(f\"The float number is {answer}\")\n",
    "    else:\n",
    "        return float(str(number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "0a724a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The float number is 12.24\n"
     ]
    }
   ],
   "source": [
    "## call it on \"$12.24\"\n",
    "test(\"$12.24\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "1e7ddf1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The float number is 10201.77\n"
     ]
    }
   ],
   "source": [
    "## call it on \"10,201.7654\"\n",
    "floater(\"10,201.7654\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "564db337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The float number is 12501.0\n"
     ]
    }
   ],
   "source": [
    "## call it on \"$12,501\"\n",
    "test(\"$12,501\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793ccf1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
